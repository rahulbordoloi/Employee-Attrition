# -*- coding: utf-8 -*-
"""Codepth.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MZTtt3RuObFuizZeswf4_RAgx9B0ImqL

# Codepth Hackathon                                                                       

Author - Rahul Bordoloi <br>
Website - [Portfolio](https://rahulbordoloi.me)           
Github - [Link](https://github.com/rahulbordoloi)                 
Email - rahulbordoloi24@gmail.com, 1729048@kiit.ac.in                          
Language & Version - Python 3.7.7      
Project Repository Link - [Github](https://github.com/rahulbordoloi/Employee-Attrition.git)

Problem Statement - 

Need to Devise a ML Algorithm which will help to predict the employees of comapany who would be leaving and control it acccordingly.

Technical Goal - <br> 

To find an optimal solution of a Classification Model. <br>

*   The goal is to find an optimal team of independent variables so that each independent variable of the team has great impact on the dependent variable profits that each independent variable of the team is a powerful predictor that is highly statistically significant.

*   Perform EDA on the datasets given and take out the insightful analysis for Research.

Assumptions -

We make the assumption that the company had given the total database of the employees before attrition.

Project Insights - (Not Done)


*   It has 50K number of invoices along with 47 vitual features of information.
*   5K Columns don't have a clearing date.
*   Range of dates invoices are from 2017-05-19 to 2019-08-06.
*   'raal9' has the highest invoice amount followed by 'Faes6'.

Downloads

*   Problem Statement - [Document Link](https://drive.google.com/file/d/1QpUC9ADne8gCi8nPrwlvEhK8NuDaWTl5/view?usp=sharing)

*   Dataset - [xlsx Link](https://drive.google.com/file/d/14h2JaexZQ2oRH1dt04FvufVu-YVIr0cv/view?usp=sharing)

*   Procedures Followed - [Document Link](https://drive.google.com/file/d/19yGEEIFxY3nae7FjMski-3uzDDUf6JMZ/view?usp=sharing)

*   Libraries Pre-requisites -  [requirements.txt](https://drive.google.com/file/d/1gz-S3CVgvZ5H81hEFlrEBMMcqddBJjsk/view?usp=sharing)        

*   Download Pre-loaded Model -  [Pickle Link](https://drive.google.com/file/d/1EOP91sWILUKdMXzfxbWk9CVODGzhyyWo/view?usp=sharing)

*   Download the iPython Notebook -  [ipynb Link](https://colab.research.google.com/drive/1MZTtt3RuObFuizZeswf4_RAgx9B0ImqL?usp=sharing)


To install , download the file and run -
```
!pip install requirements.txt
```
*   RAM of around 8GB is preferred if run on Local.

# 1. Import Dataset and Libraries
"""

from google.colab import drive
drive.mount('/content/drive')

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df_exist = pd.read_excel('/content/drive/My Drive/Codepth/DataAnalyticsProblem.xlsx', sheet_name = 'Existing employees')

df_left = pd.read_excel('/content/drive/My Drive/Codepth/DataAnalyticsProblem.xlsx', sheet_name = 'Employees who have left')

"""# 2. EDA on the Datasets"""

df_exist.head(10)

df_left.head(10)

df_exist.shape, df_left.shape

"""Inference : <br>
Before Attrition, There were a total of 14,999â€¬ Employees working for the company. <br>
And after the Attrition, 3571 employees left the company leaving behind 11428 employees.

1.  Analysis for Existing Employees.

Visualising and Dropping off the Completely Null Columns
"""

# to see how many values are missing in each column.
df_exist.isnull().sum()

# visualizing and observing the null elements in the dataset
plt.figure(figsize=(10,10))
sns.heatmap(df_exist.isnull(), cbar = False, cmap = 'YlGnBu')   # ploting missing data && # cbar, cmap = colour bar, colour map

"""Inference : <br>
There are null entries in both the datasets.

Checking for duplicate value columns
"""

x = set()                                                    # set() as to store only the unique values
for i in range(df_exist.shape[1]):
        c1 = df_exist.iloc[:, i]
        for j in range(i + 1, df_exist.shape[1]):
            c2 = df_exist.iloc[:, j]
            if c1.equals(c2):
                x.add(df_exist.columns.values[j])
for col in x:
        print(col)

"""Inference : <br>
There are No Duplicate Columns

Checking out for constant columns ie columns having same value throughout
"""

# gives out no of unqiue elements per column
df_exist.nunique()

"""Inference : <br>

1.   There are No Constant Columns.
2.   'Work_accident' and 'promotion_last_5years' are Binary Categorical Features.
3.    There are a total of 10 Departments under the Company.
4.    There are only 3 Categories for Salaries that are to be paid to 10 Departments.
"""

df_exist.info()

df_exist.describe()

df_exist.columns

print("For Work Accident : ", df_exist['Work_accident'].unique())
print("For Departments : ", df_exist['dept'].unique())
print("For Promotion in last 5 years : ", df_exist['promotion_last_5years'].unique())
print("For Salary : ", df_exist['salary'].unique())
print("For Number of Projects : ", df_exist['number_project'].unique())
print("For Time Spent in Company : ", df_exist['time_spend_company'].unique())

"""Inference : <br>

1.   The Three categories of Salaries are - 'low' 'medium' 'high'.
2.   The Employees have worked for different number of years with the years being [ 3  2  4  6  5  8 10  7] respectively.
3.   The Employees have usually worked on different projects with the numbers being [4 2 5 3 6] respectively.
"""

sns.catplot(data = df_exist, kind = 'count', x = 'promotion_last_5years')

df_exist.loc[df_exist.promotion_last_5years == 0, 'promotion_last_5years'].count(), df_exist.loc[df_exist.promotion_last_5years != 0, 'promotion_last_5years'].count()

occ = df_exist.loc[df_exist.promotion_last_5years == 0, 'promotion_last_5years'].count()
number_of_occ_per = occ/df_exist.shape[0] * 100
print(str(number_of_occ_per) + '%')

"""Inference: <br>
The Feature 'promotion_last_5years' is highly imbalanced Feature.
"""

sns.catplot(data = df_exist, kind = 'count', x = 'salary')

# df_exist['salary'].value_counts()
low = df_exist.loc[df_exist.salary == 'low', 'salary'].count()
medium = df_exist.loc[df_exist.salary == 'medium', 'salary'].count()
high = df_exist.loc[df_exist.salary == 'high', 'salary'].count()

print("No. of Employees having Low Salary are : ", low)
print("No. of Employees having Medium Salary are : ", medium)
print("No. of Employees having High Salary are : ", high)

p = [] 
p.append((low/df_exist.shape[0])*100)
p.append((medium/df_exist.shape[0])*100)
p.append((high/df_exist.shape[0])*100)

print("% of People Having 'Low' Salary : ", p[0], "%")
print("% of People Having 'Medium' Salary : ", p[1], "%")
print("% of People Having 'High' Salary : ", p[2], "%")

"""Inference : <br>
There are less Highly Paid Employees as compared to Low and Medium wage slab employees.
"""

sns.catplot(data = df_exist, kind = 'count', x = 'dept')

x = []
dept = df_exist['dept'].unique()
for i in dept:
  x.append(df_exist.loc[df_exist.dept == i, 'dept'].count())

i=0
while i < len(dept):
  #print(i)
  print("No of Employees in the Department of ",df_exist['dept'].unique()[i]," are : ",x[i])
  i+=1

"""Inference : <br>
Out of all the Departments, Sales has the most number of Employees being 3126 and the Least Number of Employees are in the Department of HR being 524 respectively.
"""

from sklearn.preprocessing import LabelEncoder
l = LabelEncoder()
df_exist.loc[:,'sal'] = l.fit_transform(df_exist.loc[:,'salary'])

df_exist.salary.unique(), df_exist.sal.unique()

# Checking out the distribution of 'salary' across different variables in Existing Employee List.
plt.figure(figsize=(25, 6))

df = pd.DataFrame(df_exist.groupby(['dept'])['sal'].mean().sort_values(ascending = False))
df.plot.bar()
plt.title('Dept. vs Salary')
plt.show()

df = pd.DataFrame(df_exist.groupby(['promotion_last_5years'])['sal'].mean().sort_values(ascending = False))
df.plot.bar()
plt.title('Promotion vs Salary')
plt.show()

x = []
x.append(df_exist.groupby(['dept'])['sal'].mean().sort_values(ascending = False))
x

x = []
x.append(df_exist.groupby(['promotion_last_5years'])['sal'].mean().sort_values(ascending = False))
x

"""Inference : <br>

1.   RandD is the Highest Paid Department followed by Support.
2.   Management is the Least Paid Department.
3.   Salary Amount has no impact on Promotion.
"""

df_exist.drop(['sal'], axis = 1, inplace = True)

x = []
dept = df_exist['dept'].unique()
for i in dept:
  y = df_exist.loc[df_exist.dept == i, 'salary']
  x.append(y.nunique())

print(x)

"""Inference : <br>
So, there's no department wise salary as such, all the departments have all the categories of salary slabs.
"""

df_exist.loc[:,'Attrition'] = int('0')

df_exist.info()

df_exist.head(10)

"""2.  Analysis for Employees who Left.

Visualising and Dropping off the Completely Null Columns
"""

# to see how many values are missing in each column.
df_left.isnull().sum()

# visualizing and observing the null elements in the dataset
plt.figure(figsize=(10,10))
sns.heatmap(df_left.isnull(), cbar = False, cmap = 'YlGnBu')   # ploting missing data && # cbar, cmap = colour bar, colour map

"""Inference : <br>
There are null entries in both the datasets.

Checking for duplicate value columns
"""

x = set()                                                    # set() as to store only the unique values
for i in range(df_left.shape[1]):
        c1 = df_left.iloc[:, i]
        for j in range(i + 1, df_left.shape[1]):
            c2 = df_left.iloc[:, j]
            if c1.equals(c2):
                x.add(df_left.columns.values[j])
for col in x:
        print(col)

"""Inference : <br>
There are No Duplicate Columns

Checking out for constant columns ie columns having same value throughout
"""

# gives out no of unqiue elements per column
df_left.nunique()

df_left.info()

df_left.describe()

df_left.columns

print("For Number of Projects : ", df_left['number_project'].unique())
print("For Time Spent in Company : ", df_left['time_spend_company'].unique())

"""Inference : <br>

1.   The Employees have worked for different number of years with the years being [3 6 4 5 2] respectively. <br>
On a Comparative Note : 

*   Employees who have left the job tend to spend more time working as compared to the existing Employees.
*   The Maximum time spent by an Employee individually is more in case of existing than the ones who left.


2.   The Employees have usually worked on different projects with the numbers being [2 5 7 6 4 3] respectively. <br>
On a Comparative Note : <br>

*   Employees who have left the job tend to have less number of projects as compared to the existing Employees.
*   The Maximum number of project performed by an Employee individually is more in case of left than the one who are existing.
"""

sns.catplot(data = df_left, kind = 'count', x = 'promotion_last_5years')

df_left.loc[df_left.promotion_last_5years == 0, 'promotion_last_5years'].count(), df_left.loc[df_left.promotion_last_5years != 0, 'promotion_last_5years'].count()

occ = df_left.loc[df_left.promotion_last_5years == 0, 'promotion_last_5years'].count()
number_of_occ_per = occ/df_left.shape[0] * 100
print(str(number_of_occ_per) + '%')

"""Inference: <br>
The Feature 'promotion_last_5years' is highly imbalanced Feature and a Quasi-Constant Feature.
"""

sns.catplot(data = df_left, kind = 'count', x = 'salary')

low = df_left.loc[df_left.salary == 'low', 'salary'].count()
medium = df_left.loc[df_left.salary == 'medium', 'salary'].count()
high = df_left.loc[df_left.salary == 'high', 'salary'].count()

print("No. of Employees who had Low Salary are : ", low)
print("No. of Employees who had Medium Salary are : ", medium)
print("No. of Employees who had High Salary are : ", high)

p = [] 
p.append((low/df_left.shape[0])*100)
p.append((medium/df_left.shape[0])*100)
p.append((high/df_left.shape[0])*100)

print("% of People who had 'Low' Salary : ", p[0], "%")
print("% of People who had 'Medium' Salary : ", p[1], "%")
print("% of People who had 'High' Salary : ", p[2], "%")

"""Inference : <br>
The % of people leaving the company were the ones with low salaries.
"""

sns.catplot(data = df_left, kind = 'count', x = 'dept')

x = []
dept = df_left['dept'].unique()
for i in dept:
  x.append(df_left.loc[df_left.dept == i, 'dept'].count())

i=0
while i < len(dept):
  #print(i)
  print("No of Employees were in the Department of ",df_left['dept'].unique()[i]," are : ",x[i])
  i+=1

"""Inference : <br>

1.   Out of all the Departments, Maximum People left the company from the Department of Sales being 1014 followed by Technical.

2.   Department of Management has the least number of attrition being 91 followed by RandD
"""

from sklearn.preprocessing import LabelEncoder
l = LabelEncoder()
df_left.loc[:,'sal'] = l.fit_transform(df_left.loc[:,'salary'])

df_left.salary.unique(), df_left.sal.unique()

# Checking out the distribution of 'salary' across different variables in Existing Employee List.
plt.figure(figsize=(25, 6))

df = pd.DataFrame(df_left.groupby(['dept'])['sal'].mean().sort_values(ascending = False))
df.plot.bar()
plt.title('Dept. vs Salary')
plt.show()

df = pd.DataFrame(df_left.groupby(['promotion_last_5years'])['sal'].mean().sort_values(ascending = False))
df.plot.bar()
plt.title('Promotion vs Salary')
plt.show()

x = []
x.append(df_left.groupby(['dept'])['sal'].mean().sort_values(ascending = False))
x

x = []
x.append(df_left.groupby(['promotion_last_5years'])['sal'].mean().sort_values(ascending = False))
x

"""Inference : <br>

1.   Amongst the employees who left, HR Dept was the Highest Paying and Support Dept was the Least.
3.   Employees who had no promotion in the last 5 years tend to quit the job more as compared to those who had promotion.
"""

df_left.drop(['sal'], axis = 1, inplace = True)

x = []
dept = df_left['dept'].unique()
for i in dept:
  y = df_left.loc[df_left.dept == i, 'salary']
  x.append(y.nunique())

print(x)

"""Inference : <br>
So, there's no department wise salary as such, all the departments have all the categories of salary slabs.
"""

df_left.loc[:,'Attrition'] = int('1')

df_left.info()

df_left.head(10)

"""# 3. Merging both the DataFrames"""

#concatenating both the datasets together
df = pd.concat([df_exist, df_left], ignore_index = False, axis = 0)

df.head(5), df.tail(5)

df.shape

"""# 4. EDA on the Merged DataFrame"""

print(df['salary'].value_counts())
print(df_left['salary'].value_counts())

print("% of 'Low' Salaried Employees that Left : ", 2172/7316*100, "%")
print("% of 'Medium' Salaried Employees that Left : ", 1317/6446*100, "%")
print("% of 'High' Salaried Employees that Left : ", 82/1237*100, "%")

sns.catplot(data = df, kind = 'count', x = 'Attrition')

sns.catplot(x = 'salary', data = df, kind='count', hue='Attrition').set_ylabels('Number of Employees')

"""Inference : <br>
So, Salary is definitely a major factor in attrition.
"""

cols = df.dept.unique()

print("***Before Attrition***")

y = []
for i in cols:
  y.append(df.loc[df.dept == i, 'dept'].count())
print(cols, "\n", y)

print('---------------')

print("***Left***")

for i in range(len(cols)):
  print(cols[i], " : ", y[i])

for i in range(len(cols)):
  print(cols[i], " : ", y[i])

for i in range(len(cols)):
  print("% of ", cols[i], " Employees that Left : ", x[i]/y[i]*100, "%")

"""Inference : <br>
Through % of employees leaving the company from their respective departments are almost same, but % is highest in case of HR which is 29% and lowest in the case of Marketing ie 14%.
"""

sns.catplot('dept', data = df, aspect = 2, kind = 'count', hue='Attrition').set_ylabels('Number of Employees')

df.columns

sns.catplot('satisfaction_level', data = df, aspect = 2, kind = 'count', hue='Attrition').set_ylabels('Number of Employees')
sns.catplot('average_montly_hours', data = df, aspect = 2, kind = 'count', hue='Attrition').set_ylabels('Number of Employees')
sns.catplot('Work_accident', data = df, aspect = 2, kind = 'count', hue='Attrition').set_ylabels('Number of Employees')
sns.catplot('time_spend_company', data = df, aspect = 2, kind = 'count', hue='Attrition').set_ylabels('Number of Employees')

"""Inference : <br>


1.   There is no such trend of Attrition in case of Work Accidents.
2.   People who have spent less time with the company tends to leave it earlier as compared to the ones who are here for a longer period of time.
3.   There's no such specific pattern as such, but the employees working highly overtime tend to leave the company.
4.   Same is the case for Satifaction Level, employees less satified with their job end to leave their job earlier.

# 5. Feature Engineering and Selection
"""

df.head(5)

df.drop(['Emp ID'], inplace = True, axis = 1)     # removing redundant feature(s)

df.shape

df.dept.value_counts()

df.salary.value_counts()

"""Inference : <br>
Since, the categorical features are not exhibiting much bias-ness, I would prefer applying K-Fold Target Encoding to map with the dependent feature.

Checking and learning about the train set's skewness.
"""

#checking the skewness of the train set
df.skew(axis = 0, skipna = True)

"""Inference : <br>

1.   Some Features seems to be a lightly right skewed.
2.   The Highly Skewed Feature is 'promotion_last_5years' followed by 'Work_accident' and 'time_spend_company'.
3.   Data here excludes the categorical columns.
4.   Skewness for dependent feature isn't to be considered.

Checking for Correlation
"""

# gives out the columns which are highly correlated amongst each other

def correlation(df, threshold = 0.90):
    corr_col = set() # Set of all the names of deleted columns
    corr_m = df.corr()
    for i in range(len(corr_m.columns)):
        for j in range(i):
            if (corr_m.iloc[i, j] >= threshold) and (corr_m.columns[j] not in corr_col):
                col = corr_m.columns[i] # getting the name of column
                corr_col.add(col)
    return corr_col

print(correlation(df,0.9))

"""Inference : <br>
There are no correlated features

# 6. Visualizations
"""

# plotting pairwise relationships in train
sns.pairplot(df)

# Distribution Plot and Boxplot to learn about features' distribution and also, to know about outliers if present.

plt.figure(figsize=(8,6))

plt.subplot(1,2,1)
plt.title('Attrition Distribution Plot')
sns.distplot(df['Attrition'])

plt.subplot(1,2,2)
plt.title('Attrition Spread')
sns.boxplot(y=df['Attrition'])

plt.show()
plt.tight_layout()

plt.figure(figsize=(8,6))

plt.subplot(1,2,1)
plt.title('Time Spent with Company Distribution Plot')
sns.distplot(df['time_spend_company'])

plt.subplot(1,2,2)
plt.title('Time Spent with Company Spread')
sns.boxplot(y=df['time_spend_company'])

plt.show()
plt.tight_layout()

"""Inference : <br>

Time Spent with Company is Highly Skewed.

# 7. Data Preprocessing
"""

df.columns

df.nunique()

df.info()

# setting up manual weightage and encoding salary
df.loc[df.salary == "low", "salary"] = 0
df.loc[df.salary == "medium", "salary"] = 1
df.loc[df.salary == "high", "salary"] = 2

df.salary.unique()

!pip install --upgrade category_encoders

from sklearn import base
from sklearn.model_selection import KFold

class KFoldTargetEncoderTrain(base.BaseEstimator, base.TransformerMixin):

    def __init__(self, colnames, targetName, n_fold=5, verbosity=True, discardOriginal_col = False):

        self.colnames = colnames
        self.targetName = targetName
        self.n_fold = n_fold
        self.verbosity = verbosity
        self.discardOriginal_col = discardOriginal_col

    def fit(self, X, y=None):
        return self
    def transform(self,X):

        assert(type(self.targetName) == str)
        assert(type(self.colnames) == str)
        assert(self.colnames in X.columns)
        assert(self.targetName in X.columns)

        mean_of_target = X[self.targetName].mean()
        kf = KFold(n_splits = self.n_fold, shuffle = False, random_state=0)

        col_mean_name = 'dept_enc'
        X[col_mean_name] = np.nan

        for tr_ind, val_ind in kf.split(X):
            X_tr, X_val = X.iloc[tr_ind], X.iloc[val_ind]
        #  print(tr_ind,val_ind)
            X.loc[X.index[val_ind], col_mean_name] = X_val[self.colnames].map(X_tr.groupby(self.colnames)[self.targetName].mean())

        X[col_mean_name].fillna(mean_of_target, inplace = True)

        if self.verbosity:

            encoded_feature = X[col_mean_name].values
            print('Correlation between the new feature, {} and, {} is {}.'.format(col_mean_name,
                                                                                      self.targetName,
                                                                                      np.corrcoef(X[self.targetName].values, encoded_feature)[0][1]))
        if self.discardOriginal_col:
            X = X.drop(self.targetName, axis=1)
            
        return X

targetc = KFoldTargetEncoderTrain('dept', 'Attrition', n_fold = 5)
df = targetc.fit_transform(df)

df.head(5)

df.drop(['dept'], inplace = True, axis = 1)

df['salary'] = pd.to_numeric(df['salary'])

df.info()

# Correlation using heatmap
plt.figure(figsize = (10, 10))
hm = df.corr().where(np.tril(np.ones(df.corr().shape)).astype(np.bool))
sns.heatmap(hm, annot = True, cmap="YlGnBu")
plt.show()

"""Splitting df into x and y i.e independent variable vector and dependent variable vector."""

x = df.drop(['Attrition'], axis = 1)
y = df.loc[:,'Attrition']

print(x.shape,y.shape)

y = y.values.reshape(-1,1)

# checking the skewness of x
x.skew(axis = 0)

"""Reducing skewness of the features according to their skewness amount."""

x.time_spend_company.unique()

# trying square-root and log transformations
log = np.log(x['time_spend_company'])
sqr = np.sqrt(x['time_spend_company'])
print(log.skew(), sqr.skew())

# Observing the distribution plot of â€˜Year of Completion of collegeâ€™ after boxcox transformation.
from scipy import stats
boxc = stats.boxcox(x['time_spend_company'])[0]
print(pd.Series(boxc).skew())
sns.distplot(boxc);

"""Inference : <br>

1. *Boxcox* will be the best transformation for 'time_spend_company'
2. Although, 'Work_accident' 'promotion_last_5years' are highly skewed, but we cannot run transformations on them as they're categorical features. as log and sqrt cannot handle '0'
"""

# Updating the required pandas series.
a = stats.boxcox(x['time_spend_company'])[0]   
x['time_spend_company'] = a

x

"""Checking for Quasi-Constant Features"""

occ = x.loc[x.promotion_last_5years == 0, 'promotion_last_5years'].count()
number_of_occ_per = occ/x.shape[0] * 100
print(str(number_of_occ_per) + '%')

occ = x.loc[x.Work_accident == 0, 'Work_accident'].count()
number_of_occ_per = occ/x.shape[0] * 100
print(str(number_of_occ_per) + '%')

"""Standard Scaling all the features to come under a common range."""

from sklearn.preprocessing import StandardScaler
sc_x = StandardScaler()
x = sc_x.fit_transform(x)

x

y

"""Inference : <br>
The Data is Imbalanced. So, we must use ensemble learning methods and cross validation to avoid overfitting.

# 8. Splitting into Train and Test Sets
"""

y.shape

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25)

print(x_train.shape,y_train.shape)

print(x_test.shape,y_test.shape)

"""# 9. Now, Model Testing!

Note : <br>
The Accuracy Metrics being used in Model Testing are - 

1.   Classification Report
2.   K-Fold Cross Validation Score
3.   Confusion Matrix

**1. Logistic Regression**
"""

# fitting simple linear regression to the training set
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state=0)
classifier.fit(x_train, y_train)

# predicting the test set results
y_pred=classifier.predict(x_test)

"""Checking Accuracies"""

from sklearn.metrics import confusion_matrix, classification_report

cm=confusion_matrix(y_test, y_pred)
plt.figure(figsize = (5,5))
sns.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')

print(classification_report(y_test, y_pred))

#applying k-fold cross validation
from sklearn.model_selection import cross_val_score as cvs
accuracies = cvs(estimator=classifier,X=x_train,y=y_train,cv=10)
print(accuracies.mean())
print(accuracies.std())

"""**2. Random Forest**"""

#fitting random forest classifier to the training set
from sklearn.ensemble import RandomForestClassifier as rfc
classifier = rfc(n_estimators=100,criterion='entropy',random_state=0)
classifier.fit(x_train, y_train)

#predicting the test set results
y_pred=classifier.predict(x_test)

"""Checking Accuracies"""

classifier.score(x_test, y_test)

from sklearn.metrics import confusion_matrix, classification_report

cm=confusion_matrix(y_test, y_pred)
plt.figure(figsize = (5,5))
sns.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')

print(classification_report(y_test, y_pred))

#applying k-fold cross validation
from sklearn.model_selection import cross_val_score as cvs
accuracies = cvs(estimator=classifier,X=x_train,y=y_train,cv=10)
print(accuracies.mean())
print(accuracies.std())

"""**3. Kernel - SVM**"""

#fitting kernel SVM to the training set
from sklearn.svm import SVC
classifier = SVC(kernel='rbf', random_state=0)
classifier.fit(x_train, y_train)

#predicting the test set results
y_pred=classifier.predict(x_test)

"""Checking Accuracies"""

from sklearn.metrics import confusion_matrix, classification_report

cm=confusion_matrix(y_test, y_pred)
plt.figure(figsize = (5,5))
sns.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')

print(classification_report(y_test, y_pred))

#applying k-fold cross validation
from sklearn.model_selection import cross_val_score as cvs
accuracies = cvs(estimator=classifier,X=x_train,y=y_train,cv=10)
print(accuracies.mean())
print(accuracies.std())

"""**4. Linear - SVM**"""

# fitting kernel SVM to the training set
from sklearn.svm import SVC
classifier = SVC(kernel='linear', random_state=0)
classifier.fit(x_train, y_train)

#predicting the test set results
y_pred=classifier.predict(x_test)

"""Checking Accuracies"""

from sklearn.metrics import confusion_matrix, classification_report

cm=confusion_matrix(y_test, y_pred)
plt.figure(figsize = (5,5))
sns.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')

print(classification_report(y_test, y_pred))

#applying k-fold cross validation
from sklearn.model_selection import cross_val_score as cvs
accuracies = cvs(estimator=classifier,X=x_train,y=y_train,cv=10)
print(accuracies.mean())
print(accuracies.std())

"""**5. K-NN**"""

#fitting knn to the training set
from sklearn.neighbors import KNeighborsClassifier as knc
classifier=knc(n_neighbors=10,metric='minkowski', p = 2)
classifier.fit(x_train, y_train)

#predicting the test set results
y_pred=classifier.predict(x_test)

"""Checking Accuracies"""

from sklearn.metrics import confusion_matrix, classification_report

cm=confusion_matrix(y_test, y_pred)
plt.figure(figsize = (5,5))
sns.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')

print(classification_report(y_test, y_pred))

#applying k-fold cross validation
from sklearn.model_selection import cross_val_score as cvs
accuracies = cvs(estimator=classifier,X=x_train,y=y_train,cv=10)
print(accuracies.mean())
print(accuracies.std())

"""**6. Decision Tree**"""

#fitting decision tree classifier to the training set
from sklearn.tree import DecisionTreeClassifier as dtc
classifier = dtc(criterion='entropy' , random_state=0)
classifier.fit(x_train, y_train)

#predicting the test set results
y_pred=classifier.predict(x_test)

"""Checking Accuracies"""

from sklearn.metrics import confusion_matrix, classification_report

cm=confusion_matrix(y_test, y_pred)
plt.figure(figsize = (5,5))
sns.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')

print(classification_report(y_test, y_pred))

#applying k-fold cross validation
from sklearn.model_selection import cross_val_score as cvs
accuracies = cvs(estimator=classifier,X=x_train,y=y_train,cv=10)
print(accuracies.mean())
print(accuracies.std())

"""**7. Naive Bayes**"""

#fitting naive bayes to the training set
from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(x_train, y_train)

#predicting the test set results
y_pred=classifier.predict(x_test)

"""Checking Accuracies"""

from sklearn.metrics import confusion_matrix, classification_report

cm=confusion_matrix(y_test, y_pred)
plt.figure(figsize = (5,5))
sns.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')

print(classification_report(y_test, y_pred))

#applying k-fold cross validation
from sklearn.model_selection import cross_val_score as cvs
accuracies = cvs(estimator=classifier,X=x_train,y=y_train,cv=10)
print(accuracies.mean())
print(accuracies.std())

"""**8. XGBoost Classifier**"""

#fitting XGBoost to the Training Set
from xgboost import XGBClassifier
classifier=XGBClassifier()
classifier.fit(x_train,y_train)

#predicting the test set results
y_pred=classifier.predict(x_test)

"""Checking Accuracies"""

from sklearn.metrics import confusion_matrix, classification_report

cm=confusion_matrix(y_test, y_pred)
plt.figure(figsize = (5,5))
sns.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')

print(classification_report(y_test, y_pred))

#applying k-fold cross validation
from sklearn.model_selection import cross_val_score as cvs
accuracies = cvs(estimator=classifier,X=x_train,y=y_train,cv=10)
print(accuracies.mean())
print(accuracies.std())

"""**9. GradientBoosting Classifier**"""

#fitting XGBoost to the Training Set
from sklearn.ensemble import GradientBoostingClassifier
classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)
classifier.fit(x_train, y_train)

#predicting the test set results
y_pred=classifier.predict(x_test)

"""Checking Accuracies"""

from sklearn.metrics import confusion_matrix, classification_report

cm=confusion_matrix(y_test, y_pred)
plt.figure(figsize = (5,5))
sns.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')

print(classification_report(y_test, y_pred))

#applying k-fold cross validation
from sklearn.model_selection import cross_val_score as cvs
accuracies = cvs(estimator=classifier,X=x_train,y=y_train,cv=10)
print(accuracies.mean())
print(accuracies.std())

"""**10. AdaBoost Classifier**"""

#fitting XGBoost to the Training Set
from sklearn.ensemble import AdaBoostClassifier
dt = dtc()
classifier = AdaBoostClassifier(n_estimators = 100, base_estimator = dt, learning_rate = 1)
classifier.fit(x_train,y_train)

#predicting the test set results
y_pred=classifier.predict(x_test)

"""Checking Accuracies"""

from sklearn.metrics import confusion_matrix, classification_report

cm=confusion_matrix(y_test, y_pred)
plt.figure(figsize = (5,5))
sns.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')

print(classification_report(y_test, y_pred))

#applying k-fold cross validation
from sklearn.model_selection import cross_val_score as cvs
accuracies = cvs(estimator=classifier,X=x_train,y=y_train,cv=10)
print(accuracies.mean())
print(accuracies.std())

"""**11. CatBoost Classifier**"""

!pip install catboost

#fitting CatBoost to the Training Set
from catboost import CatBoostClassifier
classifier = CatBoostClassifier(iterations=100, learning_rate=0.01)
classifier.fit(x_train,y_train, eval_set = (x_test, y_test))

#predicting the test set results
y_pred=classifier.predict(x_test)

"""Checking Accuracies"""

from sklearn.metrics import confusion_matrix, classification_report

cm=confusion_matrix(y_test, y_pred)
plt.figure(figsize = (5,5))
sns.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')

print(classification_report(y_test, y_pred))

#applying k-fold cross validation
from sklearn.model_selection import cross_val_score as cvs
accuracies = cvs(estimator=classifier,X=x_train,y=y_train,cv=10)
print(accuracies.mean())
print(accuracies.std())

"""**12. Light GBM**"""

!pip install lightgbm

import lightgbm as lgbm
# from sklearn import preprocessing

# kfold = KFold(n_splits=5, random_state = 0, shuffle = True)

model_lgb = lgbm.LGBMClassifier(n_iterations =50, silent = False)
model_lgb.fit(x_train, y_train)
model_lgb.score(x_test, y_test)

"""Therefore, The Accuracies of the Models are - <br>
     Model Name   -----   Accuracy f1-score-1 ----- f1-score-0
1. Logistic Regression : 0.88 0.92 0.73
2. Random Forest : 0.99 0.99 0.98
3. Kernel-SVM : 0.97 0.98 0.93
4. Liner-SVM : 0.88 0.92 0.75
5. KNN : 0.96 0.97 0.92
6. Decision Tree : 0.97 0.98 0.94
7. Naive Bayes : 0.85 0.90 0.72
8. XGBoost Classifier : 0.98 0.98 0.95
9. GradientBoosting Classifier : 0.97 0.98 0.94
10. AdaBoost Classifier : 0.98 0.98 0.95
11. CatBoost Classifier : 0.96 0.97 0.92
12. LightGBM : 0.98

# So, our Best Model Selected according to its performance is Random Forest !

# 10. Hyperparameter Tuning and Model Optimization

Using GridSearch for searching best hyperparameter.   
Model: Random Forest Classifier
"""

from sklearn.model_selection import RandomizedSearchCV,GridSearchCV   
from sklearn.model_selection import StratifiedKFold

# using grid search method to find out the best groups of hyperparameters
classifier = rfc(random_state=0)
class_cv = GridSearchCV(classifier, {"criterion":["gini","entropy"], "max_features":["auto", "sqrt", "log2"]
                            ,'max_depth': [3,4,6,8], 'n_estimators': [100,200,500], }, verbose=1)
class_cv.fit(x_train,y_train)

#dictionary of the best parameters
class_cv.best_params_

classifier.get_params()

"""Train data using Random Forest with best parameters"""

classification = rfc(**class_cv.best_params_)
classifier.fit(x_train,y_train)

"""Predicting the Results."""

y_pred = classifier.predict(x_test)
y_pred

"""Evaluating its Score"""

from sklearn.metrics import confusion_matrix, classification_report

cm=confusion_matrix(y_test, y_pred)
plt.figure(figsize = (5,5))
sns.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')

"""***Classification Report Before Hyperparameter Tuning***


```
                precision    recall  f1-score   support

           0       0.99      1.00      0.99      2859
           1       0.99      0.96      0.98       891

    accuracy                           0.99      3750
   macro avg       0.99      0.98      0.98      3750
weighted avg       0.99      0.99      0.99      3750
```
"""

print("***Classification Report After Hyperparameter Tuning***")
print("\n")
print(classification_report(y_test, y_pred))

# applying k-fold cross validation
from sklearn.model_selection import cross_val_score as cvs
accuracies = cvs(estimator=classifier,X=x_train,y=y_train,cv=10)
print(accuracies.mean())
print(accuracies.std())

"""Inference : <br>
So, there's a slight improvement in our Model After HyperParameter Tuning. 
Recall has been increased by 0.1% which is great given our dataset was imbalanced. <br>
<br>
So, Finally Our Overall Model Accuracy Stands at - 99%

with, 99% Accuracy on '0' Label and <br>
      98% Accuracy on '1' Label, which was our Prime Goal.

# 11. Saving the Model
"""

import pickle
filename = 'Codepth.pkl'
pickle.dump(classifier, open(filename, 'wb'))

"""# 12. Creating a ML Pipeline

Note : 'boxcox' transformation has not been performed here as no 'column transformer' has been designed till yet that can handle 'boxcox' transformations and fit it.
"""

from sklearn.pipeline import Pipeline

pipe = Pipeline([('standard', StandardScaler()),
                    #('boxcox'), stats.boxcox()),
                    ('randomforest', rfc(**class_cv.best_params_))])

pipe.fit(x_train, y_train)

score = pipe.score(x_test, y_test)
print('Random Forest Pipeline Test Accuracy: %.3f' % score)

"""Note : There's a reduce in accuracy as 'boxcox' transformation was not performed in the pipeline.

# 13. Generating Requirements File
"""

!pip freeze > requirements.txt

"""# End"""